prob <- predict(fit, type = "probs")[, 1L, drop = TRUE]
getJitteredResiduals <- function(object, jitter.scale, y) {
prob <- predict(object, type = "probs")[, 1L, drop = TRUE]
y <- as.integer(y) - 1
if (jitter.scale == "response") {
runif(length(y), min = y, max = y + 1) - (prob + 0.5)
} else {
.min <- pbinom(y - 1, size = 1, prob = prob)  # F(y-1)
.max <- pbinom(y, size = 1, prob = prob)  # F(y)
runif(length(y), min = .min, max = .max) - 0.5  # S|Y=y - E(S|X)
}
}
par(mfrow = c(1, 2))
res1 <- getJitteredResiduals(fit, "probability", df1$y)
plot(df1$x, res1)
res2 <- getJitteredResiduals(fit, "response", df1$y)
plot(df1$x, res2)
?rbinom
#' @keywords internal
getResponseValues.multinom <- function(object) {
as.integer(model.response(model.frame(object))) - 1  # convert to 0, 1
}
getResponseValues.multinom(fit)
source('C:/Users/greenweb/Desktop/sure-multinom.R', echo=TRUE)
source('C:/Users/greenweb/Desktop/sure-multinom.R', echo=TRUE)
source('C:/Users/greenweb/Desktop/sure-multinom.R', echo=TRUE)
getProbs(fit)
fit
source('C:/Users/greenweb/Desktop/sure-multinom.R', echo=TRUE)
source('C:/Users/greenweb/Desktop/sure-multinom.R', echo=TRUE)
plot(df1)
devtools::install_github("AFIT-R/MODA")
################################################################################
# Setup
################################################################################
# Load required packages
library(ggplot2)
library(MASS)
library(ordinal)
library(rms)
library(VGAM)
library(sure)
data(df1)
# Fit a (correct) probit model
fit.polr <- polr(y ~ x + I(x ^ 2), data = df1, method = "probit")
# Probability-scale residuals
pres <- presid(fit.polr)
# Residual plots using the SBS residuals
p1 <- ggplot(data.frame(x = df1$x, y = pres), aes(x, y)) +
geom_point(alpha = 0.5) +
geom_smooth(color = "red", se = FALSE) +
ylab("Probability residual")
p2 <- ggplot(data.frame(y = pres), aes(sample = y)) +
stat_qq(distribution = qunif, dparams = list(min = -1, max = 1), alpha = 0.5) +
xlab("Sample quantile") +
ylab("Theoretical quantile")
# Figure ?
pdf(file = "manuscript\\quadratic-correct-sbs.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
library(PResiduals)
# Load the simulated quadratic data
data(df1)
# Fit a (correct) probit model
fit.polr <- polr(y ~ x + I(x ^ 2), data = df1, method = "probit")
# Probability-scale residuals
pres <- presid(fit.polr)
# Residual plots using the SBS residuals
p1 <- ggplot(data.frame(x = df1$x, y = pres), aes(x, y)) +
geom_point(alpha = 0.5) +
geom_smooth(color = "red", se = FALSE) +
ylab("Probability residual")
p2 <- ggplot(data.frame(y = pres), aes(sample = y)) +
stat_qq(distribution = qunif, dparams = list(min = -1, max = 1), alpha = 0.5) +
xlab("Sample quantile") +
ylab("Theoretical quantile")
# Figure ?
pdf(file = "manuscript\\quadratic-correct-sbs.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
# Surrogate residuals
set.seed(101)  # for reproducibility
sres <- resids(fit.polr)
# Residual plots using the surrogate-based residuals
p1 <- autoplot(sres, what = "covariate", x = df1$x, xlab = "x")
p2 <- autoplot(sres, what = "qq", disttribution = pnorm)
getwd()
# Residual plots using the SBS residuals
p1 <- ggplot(data.frame(x = df1$x, y = pres), aes(x, y)) +
geom_point(alpha = 0.5) +
geom_smooth(color = "red", se = FALSE) +
ylab("Probability residual")
p2 <- ggplot(data.frame(y = pres), aes(sample = y)) +
stat_qq(distribution = qunif, dparams = list(min = -1, max = 1), alpha = 0.5) +
xlab("Sample quantile") +
ylab("Theoretical quantile")
# Figure ?
pdf(file = "quadratic-correct-sbs.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
# Surrogate residuals
set.seed(101)  # for reproducibility
sres <- resids(fit.polr)
# Residual plots using the surrogate-based residuals
p1 <- autoplot(sres, what = "covariate", x = df1$x, xlab = "x")
p2 <- autoplot(sres, what = "qq", disttribution = pnorm)
# Figure ?
pdf(file = "quadratic-correct-surrogate.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
table(df1$y)
summary(1:10)
fivenum(1:10)
x <- c(5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)
hist(x)
sd(x)
var(x)
mean(x)
median(x)
IQR(x)
mad(x)
sd(rep(1, 10), rep(5, 10))
sd(c(rep(1, 10), rep(5, 10)))
sd(c(5, 10))
install.packages("ISLR")
data(Caravan)
??Caravan
head(Caravan)
head(Caravan)
library(ISLR)
head(Caravan)
library(randomForest)
fit <- randomForest(Purchase ~ ., data = Caravan, importance = TRUE)
?randomForest
fit <- randomForest(Purchase ~ ., data = Caravan, importance = TRUE, do.trace = TRUE)
library(vip)
varImpPlot(fit)
vip(fit, progress = "text", quantiles = TRUE, probs = 0:10/10)
vip(fit, progress = "text", quantiles = TRUE, probs = 0:10/10, pred.var = subset(Caravan, select = -Purchases))
vip(fit, progress = "text", quantiles = TRUE, probs = 0:10/10, pred.var = subset(Caravan, select = -Purchase))
pred.var = subset(Caravan, select = -Purchase)
pred.var
vip(fit, progress = "text", quantiles = TRUE, probs = 0:10/10, pred.var = names(subset(Caravan, select = -Purchase)))
fit
plot(fit)
head(df2)
library(sure)
head(df2)
library(ordinal)
?clm
# Fit a cumulative link model with probit link
fit.clm <- clm(y ~ x, data = df2, link = "probit")
# Obtain the SBS/probability-scale residusls
pres <- PResiduals::presid(fit.polr)
# Residual vs. covariate plots
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.clm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = hd$x, y = ls.res), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
library(ggplot2)
library(ordinal)
# Fit a cumulative link model with probit link
fit.clm <- clm(y ~ x, data = df2, link = "probit")
# Obtain the SBS/probability-scale residusls
pres <- PResiduals::presid(fit.polr)
# Residual vs. covariate plots
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.clm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = hd$x, y = ls.res), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
pres <- PResiduals::presid(fit.clm)
?PResiduals::presid
library(rms)
?orm
fit.orm <- orm(y ~ x, data = df2, family = "probit")
pres <- presid(fit.clm)
library(PResiduals)
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.clm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = hd$x, y = presid(fit.clm)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.clm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = df2$x, y = presid(fit.clm)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.orm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = df2$x, y = presid(fit.orm)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
# Residual vs. covariate plots
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.orm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = df2$x, y = presid(fit.orm, x = TRUE)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.orm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = df2$x, y = presid(fit.orm)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
fit.orm <- orm(y ~ x, data = df2, family = "probit", x = TRUE)
# Residual vs. covariate plots
set.seed(102)  # for reproducibility
p1 <- autoplot(fit.orm, what = "covariate", x = df2$x, xlab = "x")
p2 <-   ggplot(data.frame(x = df2$x, y = presid(fit.orm)), aes(x, y)) +
geom_point(size = 2, alpha = 0.25) +
geom_smooth(col = "red", se = FALSE) +
ylab("Probability scale residual")
grid.arrange(p1, p2, ncol = 2)
# Figure ?
pdf(file = "manuscript\\heteroscedasticity.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
pdf(file = "heteroscedasticity.pdf", width = 8, height = 4)
grid.arrange(p1, p2, ncol = 2)
dev.off()
?presid
library(bootstrap)
install.packages("bootstrap")
library(doParallel) # load the parallel backend
cl <- makeCluster(4) # use 4 workers
registerDoParallel(cl) # register the parallel backend
library(pdp)
library(xgboost)
data(boston, package = "pdp")
boston <- sapply(boston, as.numeric)
x<-as.matrix(subset(boston,,-c(cmedv)))
y<-as.matrix(subset(boston,,c(cmedv)))
dtrain <- xgb.DMatrix(data =x, label =y )
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 50, objective = "reg:linear", verbose = 2)
xgb.save(bst,'bst.mod')
mod_load<-xgb.load('bst.mod')
mod_load %>% partial(train = x,pred.var = "lstat", grid.resolution = 15,parallel = TRUE) %>% pdp::plotPartial(smooth=TRUE, ylab =expression(f(lstat)), xlab = "lstat", main="Partial Dependence Plot \n -with smooth-")
mod_load
names(mod_load)
mod_load$handle
names(mod_load$handle)
names(mod_load$raw)
?xgb.load
predict.xgb
class(mod_load)
class(bst)
names(bst)
?xgboost
`xgb.parameters(bst)
))
()
''
"
<- NULL
NULL
md
)))
?xgb.save
library(doParallel) # load the parallel backend
cl <- makeCluster(4) # use 4 workers
registerDoParallel(cl) # register the parallel backend
library(pdp)
library(xgboost)
data(boston, package = "pdp")
boston <- sapply(boston, as.numeric)
x<-as.matrix(subset(boston,,-c(cmedv)))
y<-as.matrix(subset(boston,,c(cmedv)))
dtrain <- xgb.DMatrix(data =x, label =y )
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 50, objective = "reg:linear", verbose = 2)
#OK
bst %>% partial(train = x,pred.var = "lstat", grid.resolution = 15,parallel = TRUE) %>%
pdp::plotPartial(smooth=TRUE, ylab =expression(f(lstat)),
xlab = "lstat", main="Partial Dependence Plot \n -with smooth-")
xgb.save(bst,'bst.mod')
mod_load<-xgb.load('bst.mod')
?xgb.save
mod_load %>% partial(train = x,pred.var = "lstat", grid.resolution = 15,parallel = TRUE) %>% pdp::plotPartial(smooth=TRUE, ylab =expression(f(lstat)), xlab = "lstat", main="Partial Dependence Plot \n -with smooth-")
#fails
mod_load %>% partial(train = x,pred.var = "lstat", grid.resolution = 15,parallel = TRUE,
type = "regression") %>%
pdp::plotPartial(smooth=TRUE, ylab =expression(f(lstat)), xlab = "lstat", main="Partial Dependence Plot \n -with smooth-")
install.packages("mgcv")
librp
library(pdp)
?partial
library(randomForest)
data (boston)  # load the boston housing data
set.seed(101)  # for reproducibility
boston.rf <- randomForest(cmedv ~ ., data = boston)
partial(boston.rf, pred.var = c("lstat", "rm"), grid.resolution = 40,
plot = TRUE, chull = TRUE, progress = "text")
?mgcv::in.out
data(package = "ordinal")
?ordinal::soup
?ordinal::income
?df4
library(sure)
?df4
head(df4)
head(df4, 50)
tail(df4, 50)
table(df4$y)
library("tmvtnorm")
library("VGAM")
set.seed(977)
###############################################################
### The function used to ordinalize a continous variable
### Ordinal value: 1,2,3...
### Inputs:
### 1. "z" is a vector to be ordinalized;
### 2. "threshold" is a vector specifying the cut-off points
### Output: the returned value is also a vector
###############################################################
ordinalize<-function(z,threshold){
oo<-function(zz){
ordinal.value<-1;index<-1
while(index<=length(threshold) && zz>threshold[index]) {ordinal.value<-ordinal.value+1;index<-index+1}
return(ordinal.value)
}
return(sapply(z,oo))
}
#########################################################################
### Simulate latent variables Z1 aqnd Z2
### The ordinal outcome W is obtained by discretizing Z
########################################################################
n<-2000
alpha<-0;beta_1<--1
thrd1<-c(-1.5,0)
X<-runif(n,-3,3)
Z1<-alpha+beta_1*X+rnorm(n)
plot(Z1~X)
abline(h=thrd1[1],col="red")
abline(h=thrd1[2],col="red")
# abline(h=thrd[3],col="red")
# abline(h=thrd[4],col="red")
W1<-ordinalize(Z1,thrd1)
table(W1)
alpha<-0;beta_2<--1.5
thrd2<-c(1,3)
Z2<-alpha+beta_2*X+rnorm(n)
W2<-ordinalize(Z2,thrd2)
table(W2)
model1<-vglm(formula=(W1-1)~X,family = cumulative(link = probit,parallel = TRUE))
beta1.hat<--coef(model1)[3]
thrd1.hat<-c(coef(model1)[1],coef(model1)[2])
##### surrogate variable
surrogate.bootstrap<-function(w,x,beta.hat,thrd.hat){
cc<-c(-Inf,thrd.hat,Inf)
S<-rtmvnorm(1,mean=(beta.hat*x),sigma=1,lower=cc[w],upper=cc[w+1])
return(S)
}
S1.boot<-rep(NA,n)
for(i in 1:n){S1.boot[i]<-surrogate.bootstrap(W1[i],X[i],beta1.hat,thrd1.hat)}
windows()
plot(S1.boot~X)
model2<-vglm(formula=(W2-1)~X,family = cumulative(link = probit,parallel = TRUE))
beta2.hat<--coef(model2)[3]
thrd2.hat<-c(coef(model2)[1],coef(model2)[2])
S2.boot<-rep(NA,n)
for(i in 1:n){S2.boot[i]<-surrogate.bootstrap(W2[i],X[i],beta2.hat,thrd2.hat)}
windows()
plot(S2.boot~X)
windows()
D<-S2.boot-S1.boot
plot(D~X)
scatter.smooth(X,D, span = 0.5,degree=2,lpars = list(col = "red", lwd = 3),ylab="D")
abline(h=0,lty=2)
source('C:/Users/greenweb/Downloads/Proportionality.r', echo=TRUE)
table(W1)
table(W2)
library(VGAM)
fit1 <- vglm(y ~ x, data = df4[1:2000, ],
cumulative(link = probit, parallel = TRUE)
fit2 <- update(fit1, data = df4[2001:4000, ])
fit2 <- update(fit1, data = df4[2001:4000, ])
library(VGAM)
fit1 <- vglm(y ~ x, data = df4[1:2000, ],
cumulative(link = probit, parallel = TRUE)
)
fit1 <- vglm(y ~ x, data = df4[1:2000, ],
cumulative(link = probit, parallel = TRUE))
fit2 <- update(fit1, data = df4[2001:4000, ])
library(ggplot2)
library(VGAM)
fit1 <- vglm(y ~ x, data = df4[1:2000, ],
cumulative(link = probit, parallel = TRUE))
fit2 <- update(fit1, data = df4[2001:4000, ])
s1 <- surrogate(fit1)
s2 <- surrogate(fit2)
d <- data.frame(D = s1 - s2, X = df4[1:2000, ]$x)
ggplot(d, aes(x = X, y = D)) +
geom_point() +
geom_smooth()
d <- data.frame(D = s1 - s2, X = df4[2001:4000, ]$x)
ggplot(d, aes(x = X, y = D)) +
geom_point() +
geom_smooth(se = FALSE)
# Fit separate models to the df4 data set and genrate the difference in
# surrogate values
library(VGAM)
fit1 <- vglm(y ~ x, data = df4[1:2000, ],
cumulative(link = probit, parallel = TRUE))
fit2 <- update(fit1, data = df4[2001:4000, ])
s1 <- surrogate(fit1)
s2 <- surrogate(fit2)
d <- data.frame(D = s1 - s2, X = df4[1:2000, ]$x)
# Scatterplot of D vs. X
p <- ggplot(d, aes(x = X, y = D)) +
geom_point() +
geom_smooth(col = "red", se = FALSE)
# Figure ?
pdf(file = "proportionality.pdf", width = 7, height = 5)
print(p)
dev.off()
table(W1)
table(W2)
fit1
fit2
?df3
exp(2) - exp(1 / 2)
plnorm(2)
dlnorm(2)
# Fit models with various link functions to the simulated data
fit.probit <- polr(y ~ x + I(x ^ 2), data = df3, method = "probit")
fit.logistic <- polr(y ~ x + I(x ^ 2), data = df3, method = "logistic")
fit.loglog <- polr(y ~ x + I(x ^ 2), data = df3, method = "loglog")  # correct link
fit.cloglog <- polr(y ~ x + I(x ^ 2), data = df3, method = "cloglog")
# Construc Q-Q plots of the surrogate residuals for each model
p1 <- autoplot(fit.probit, nsim = 100, what = "qq")
p2 <- autoplot(fit.logistic, nsim = 100, what = "qq")
p3 <- autoplot(fit.loglog, nsim = 100, what = "qq")
p4 <-  autoplot(fit.cloglog, nsim = 100, what = "qq")
library(MASS)
# Fit models with various link functions to the simulated data
fit.probit <- polr(y ~ x + I(x ^ 2), data = df3, method = "probit")
fit.logistic <- polr(y ~ x + I(x ^ 2), data = df3, method = "logistic")
fit.loglog <- polr(y ~ x + I(x ^ 2), data = df3, method = "loglog")  # correct link
fit.cloglog <- polr(y ~ x + I(x ^ 2), data = df3, method = "cloglog")
# Construc Q-Q plots of the surrogate residuals for each model
p1 <- autoplot(fit.probit, nsim = 100, what = "qq")
p2 <- autoplot(fit.logistic, nsim = 100, what = "qq")
p3 <- autoplot(fit.loglog, nsim = 100, what = "qq")
p4 <-  autoplot(fit.cloglog, nsim = 100, what = "qq")
grid.arrange(p1, p2, p3, p4, ncol = 2)  # bottom left plot is correct model
pdf(file = "link.pdf", width = 7, height = 5)
grid.arrange(p1, p2, p3, p4, ncol = 2)  # bottom left plot is correct model
dev.off()
pdf(file = "link.pdf", width = 7, height = 7)
grid.arrange(p1, p2, p3, p4, ncol = 2)  # bottom left plot is correct model
dev.off()
plot(gof(fit.cloglog))
plot(gof(fit.loglog))
plot(gof(fit.loglog, nsim = 50))
plot(gof(fit.cloglog, nsim = 50))
plot(gof(fit.probit, nsim = 50))
plot(gof(fit.logistic, nsim = 50))
plot(gof(fit.loglog, nsim = 50))
plot(gof(fit.loglog, nsim = 500))
plot(gof(fit.logistic, nsim = 500))
?gof
?ks.test
?wine
?ordinal::wine
x <- c("Abc", "DEFF", "AJKJdjdksKDJ")
tolower(x)
library(xlsx)
library(readxl)
?read_xlsx
onelines <- read_excel(file.choose(), sheet = 1, skip = 2, col_names = TRUE)
head(onelines)
library(tm)
library(tidytext)
vs <- VectorSource(onelines$One.liner)
names(onelines)
names(onelines) <- "One.liner"
vs <- VectorSource(onelines$One.liner)
vs
dt <- DocumentTermMatrix(vs)
inspect(vs)
?VCorpus
vc <- VCorpus(vs)
inspect(vc)
inspect(vc[[1]])
citation(package = "sure")
citation(package = "anomalyDetection")
?alpha.f
?adjustcolor
library(ggplot2)
library(MASS)
library(ordinal)
library(PResiduals)
library(rms)
library(VGAM)
library(sure)
data(wine, package = "ordinal")
wine.clm <- clm(rating ~ temp * contact, data = wine)  # default logit link
grid.arrange(
autoplot(wine.clm, nsim = 10, what = "qq"),
autoplot(wine.clm, nsim = 10, what = "fitted"),
autoplot(wine.clm, nsim = 10, what = "cov", x = wine$temp),
autoplot(wine.clm, nsim = 10, what = "cov", x = wine$contact),
ncol = 2
)
?VGAM::wine
